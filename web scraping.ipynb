{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping :\n",
    "The process of extracting data from websites by parsing their HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, install beautifulsoup4 and an HTML parser like lxml. Alternatively, you can use other parsers like html5lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Working with a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\" integrity=\"sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z\" rel=\"stylesheet\"/>\n",
      "  <title>\n",
      "   Courses Overview\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Welcome to Your Learning Path!\n",
      "  </h1>\n",
      "  <div class=\"card\" id=\"card-python-intro\">\n",
      "   <div class=\"card-header\">\n",
      "    Python\n",
      "   </div>\n",
      "   <div class=\"card-body\">\n",
      "    <h5 class=\"card-title\">\n",
      "     Python for Beginners\n",
      "    </h5>\n",
      "    <p class=\"card-text\">\n",
      "     If you're just starting out with programming, this course will guide you through Python basics!\n",
      "    </p>\n",
      "    <a class=\"btn btn-primary\" href=\"#\">\n",
      "     Start for $19\n",
      "    </a>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"card\" id=\"card-python-web-dev\">\n",
      "   <div class=\"card-header\">\n",
      "    Python\n",
      "   </div>\n",
      "   <div class=\"card-body\">\n",
      "    <h5 class=\"card-title\">\n",
      "     Python Web Development\n",
      "    </h5>\n",
      "    <p class=\"card-text\">\n",
      "     Build your first website using Python! Learn how to create web applications with Flask and Django.\n",
      "    </p>\n",
      "    <a class=\"btn btn-primary\" href=\"#\">\n",
      "     Start for $49\n",
      "    </a>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"card\" id=\"card-python-advanced\">\n",
      "   <div class=\"card-header\">\n",
      "    Python\n",
      "   </div>\n",
      "   <div class=\"card-body\">\n",
      "    <h5 class=\"card-title\">\n",
      "     Advanced Python Programming\n",
      "    </h5>\n",
      "    <p class=\"card-text\">\n",
      "     Master Python and tackle complex problems in software development, data analysis, and automation.\n",
      "    </p>\n",
      "    <a class=\"btn btn-primary\" href=\"#\">\n",
      "     Start for $99\n",
      "    </a>\n",
      "   </div>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "<h5 class=\"card-title\">\n",
      " Python for Beginners\n",
      "</h5>\n",
      "\n",
      "Python for Beginners\n",
      "Python Web Development\n",
      "Advanced Python Programming\n",
      "Python for Beginners $19\n",
      "Python Web Development $49\n",
      "Advanced Python Programming $99\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ok , here we are going to work with a local html file , so no need for request, we just read the html file\n",
    "with open ('home.html', 'r') as file :\n",
    "    content = file.read()\n",
    "    \n",
    "# now we turn it into a beautifulsoup object\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# printing in a readable format \n",
    "print(soup.prettify())\n",
    "\n",
    "# find the first element in a specefic tag\n",
    "tag = soup.find('h5')\n",
    "# tag is a beatiful soup object so we can apply the methodes , here I printed the html code \n",
    "print(tag.prettify())\n",
    "\n",
    "# find all element with that specefic tag, give a coherent name\n",
    "courses_names_tags = soup.find_all('h5')\n",
    "\n",
    "# here courses_names_tags is a list of beautifulsoup objects , so we iterate and print\n",
    "for item in courses_names_tags:\n",
    "    print(item.text)\n",
    "    \n",
    "# start wit the tag , then the class \n",
    "course_cards=soup.find_all('div', class_ = 'card')\n",
    "for course in course_cards:\n",
    "    course_name = course.h5.text  # do .tag.thing u want\n",
    "    course_price = course.a.text.split()[-1]  \n",
    "    print(course_name, course_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Working with a real website\n",
    "\n",
    "Web scraping involves making requests to websites to get their HTML, then parsing it to extract the desired data.\n",
    "\n",
    "Tools like Beautiful Soup make it easier to parse the HTML compared to using basic text manipulation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Just: Just a Command Runner, Site: (just.systems)\n",
      "Points: 250 points, Author: thunderbong, Time: 5 hours ago, Comments: 159 comments\n",
      "\n",
      "Raspberry Pi 5 now supports Valve's Steam Link, Site: (raspberrypi.com)\n",
      "Points: 126 points, Author: Venn1, Time: 2 hours ago, Comments: 31 comments\n",
      "\n",
      "MIT largest open-source car design dataset, incl aerodynamics, to speed design, Site: (news.mit.edu)\n",
      "Points: 114 points, Author: toss1, Time: 7 hours ago, Comments: 32 comments\n",
      "\n",
      "I algorithmically donated $5000 to Open Source, Site: (kvinogradov.com)\n",
      "Points: 260 points, Author: lorey, Time: 10 hours ago, Comments: 51 comments\n",
      "\n",
      "The Startup Trap (2013), Site: (cleancoder.com)\n",
      "Points: 31 points, Author: sandwichsphinx, Time: 3 hours ago, Comments: 24 comments\n",
      "\n",
      "Beekeepers halt honey awards over fraud in global supply chain, Site: (theguardian.com)\n",
      "Points: 115 points, Author: a_w, Time: 6 hours ago, Comments: 108 comments\n",
      "\n",
      "An EPYC Exclusive for Azure: AMD's MI300C – By George Cozma, Site: (chipsandcheese.com)\n",
      "Points: 37 points, Author: rbanffy, Time: 5 hours ago, Comments: 9 comments\n",
      "\n",
      "Show HN: Countless.dev – A website to compare every AI model: LLMs, TTSs, STTs, Site: (countless.dev)\n",
      "Points: 194 points, Author: ahmetd, Time: 12 hours ago, Comments: 52 comments\n",
      "\n",
      "Windows on ARM Gets Major Gaming Boost with Prism Update, Site: (windowsonarm.org)\n",
      "Points: 20 points, Author: aviat, Time: 4 hours ago, Comments: 1 comment\n",
      "\n",
      "Every V4 UUID, Site: (everyuuid.com)\n",
      "Points: 1287 points, Author: LorenDB, Time: 1 day ago, Comments: 344 comments\n",
      "\n",
      "Historically, 4NF explanations are needlessly confusing, Site: (minimalmodeling.substack.com)\n",
      "Points: 24 points, Author: thunderbong, Time: 6 hours ago, Comments: 2 comments\n",
      "\n",
      "The momentum of the solar energy transition, Site: (nature.com)\n",
      "Points: 27 points, Author: simonebrunozzi, Time: 1 hour ago, Comments: 3 comments\n",
      "\n",
      "Nightclub stickers over smartphone rule divides the dancefloor, Site: (bbc.co.uk)\n",
      "Points: 19 points, Author: mellosouls, Time: 1 hour ago, Comments: 16 comments\n",
      "\n",
      "Hiroshi Nagai: Japan's Sun-Drenched Americana, Site: (tokyocowboy.co)\n",
      "Points: 166 points, Author: neom, Time: 14 hours ago, Comments: 26 comments\n",
      "\n",
      "Google's AI weather prediction model is pretty darn good, Site: (theverge.com)\n",
      "Points: 52 points, Author: Garbage, Time: 3 hours ago, Comments: 30 comments\n",
      "\n",
      "US Food and Drug Administration moves to ban red food dye, Site: (theguardian.com)\n",
      "Points: 23 points, Author: rntn, Time: 48 minutes ago, Comments: 6 comments\n",
      "\n",
      "Next-level frosted glass with backdrop-filter, Site: (joshwcomeau.com)\n",
      "Points: 352 points, Author: timdorr, Time: 23 hours ago, Comments: 61 comments\n",
      "\n",
      "My second year without a job, Site: (shilin.ca)\n",
      "Points: 426 points, Author: true_pk, Time: 1 day ago, Comments: 667 comments\n",
      "\n",
      "Melons and Melancholy: \"Eating and Being\" illuminates the dynamics of dietetics, Site: (lareviewofbooks.org)\n",
      "Points: 3 points, Author: crescit_eundo, Time: 2 hours ago, Comments: discuss\n",
      "\n",
      "Five of the best science fiction books of 2024, Site: (theguardian.com)\n",
      "Points: 18 points, Author: sorokod, Time: 1 hour ago, Comments: discuss\n",
      "\n",
      "Ultralytics AI model hijacked to infect thousands with cryptominer, Site: (bleepingcomputer.com)\n",
      "Points: 39 points, Author: sandwichsphinx, Time: 3 hours ago, Comments: 12 comments\n",
      "\n",
      "Accessing a DRM Framebuffer to display an image, Site: (embear.ch)\n",
      "Points: 62 points, Author: compressedgas, Time: 14 hours ago, Comments: 7 comments\n",
      "\n",
      "Can life emerge around a white dwarf?, Site: (centauri-dreams.org)\n",
      "Points: 92 points, Author: JPLeRouzic, Time: 14 hours ago, Comments: 66 comments\n",
      "\n",
      "Biggest shell programs, Site: (github.com/oils-for-unix)\n",
      "Points: 178 points, Author: todsacerdoti, Time: 21 hours ago, Comments: 90 comments\n",
      "\n",
      "<dialog>: The Dialog Element, Site: (developer.mozilla.org)\n",
      "Points: 309 points, Author: htunnicliff, Time: 1 day ago, Comments: 139 comments\n",
      "\n",
      "Lies I was told about collab editing, Part 1: Algorithms for offline editing, Site: (moment.dev)\n",
      "Points: 260 points, Author: antics, Time: 1 day ago, Comments: 92 comments\n",
      "\n",
      "SpiderBasic, Site: (spiderbasic.com)\n",
      "Points: 52 points, Author: shakna, Time: 12 hours ago, Comments: 28 comments\n",
      "\n",
      "Protecting undersea internet cables is a tech nightmare, Site: (ieee.org)\n",
      "Points: 81 points, Author: pseudolus, Time: 6 hours ago, Comments: 85 comments\n",
      "\n",
      "Mathics 7.0 – Open-source alternative to Mathematica, Site: (github.com/mathics3)\n",
      "Points: 121 points, Author: adius, Time: 9 hours ago, Comments: 22 comments\n",
      "\n",
      "Mistakes as a new manager, Site: (terriblesoftware.org)\n",
      "Points: 273 points, Author: Sharpie4679, Time: 1 day ago, Comments: 116 comments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://news.ycombinator.com/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "print(response.status_code)\n",
    "\n",
    "# Parse the response content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Extract the titles and subtitles from the website\n",
    "news_titles = soup.find_all('span', class_='titleline')\n",
    "news_subtitles = soup.find_all('span', class_='subline')\n",
    "\n",
    "# Link between the title and other info:\n",
    "for title, subtitle in zip(news_titles, news_subtitles):\n",
    "    item_site = title.text.split()[-1]  # Get the last part of the title as the site\n",
    "    item_title = ' '.join(title.text.split()[:-1])  \n",
    "    news_points = subtitle.find('span', class_='score').text\n",
    "    news_author = subtitle.find('a', class_='hnuser').text\n",
    "    news_time_posted = subtitle.find('span', class_='age').text\n",
    "    news_comments = subtitle.find_all('a')[-1].text\n",
    "    # Print combined info\n",
    "    print(f\"{item_title}, Site: {item_site}\")\n",
    "    print(f\"Points: {news_points}, Author: {news_author}, Time: {news_time_posted}, Comments: {news_comments}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding a Filter for Minimum Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure valid input for min_points\n",
    "while True:\n",
    "    try:\n",
    "        min_points = int(input('Enter the minimum number of points: '))\n",
    "        break  # Exit the loop if a valid integer is provided\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number.\")\n",
    "\n",
    "def find_news(min_points):\n",
    "    '''\n",
    "    logic to filter the articles based on points :\n",
    "    \n",
    "    for title, subtitle in zip(news_titles, news_subtitles):\n",
    "    \n",
    "        # We get the number of points and turn them into an integer \n",
    "        news_points = int(subtitle.find('span', class_='score').text.strip().split()[0]) \n",
    "        \n",
    "        # select article with more points\n",
    "        if news_points >= min_points:\n",
    "        \n",
    "        # parse info\n",
    "            item_site = title.text.split()[-1]  # Get the site (last part of the title)\n",
    "            item_title = ' '.join(title.text.split()[:-1])  # Join everything except the last word (site)\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "find_news(min_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraped_data.csv', 'w', newline='') as f:\n",
    "    f.write('Title,website,points,author,time posted,number of comments\\n')\n",
    "\n",
    "    # Replace special characters with a space\n",
    "    def clean_text(text):\n",
    "        return text.replace(',', ' ').replace('�', ' ').replace('–', ' ').replace('•', ' ')\n",
    "    \n",
    "    for title, subtitle in zip(news_titles, news_subtitles):\n",
    "        news_points = int(subtitle.find('span', class_='score').text.strip().split()[0])  \n",
    "        \n",
    "        if news_points >= min_points:\n",
    "            item_site = clean_text(title.text.split()[-1])  # Clean site (last part of the title)\n",
    "            item_title = clean_text(' '.join(title.text.split()[:-1]))  # Clean title\n",
    "            news_author = clean_text(subtitle.find('a', class_='hnuser').text)  # Clean author\n",
    "            news_time_posted = clean_text(subtitle.find('span', class_='age').text)  # Clean time posted\n",
    "            news_comments = clean_text(subtitle.find_all('a')[-1].text.strip())\n",
    "            news_comments = int(news_comments.split()[0])  # Ensure we only take the numeric part\n",
    "            \n",
    "            # Write each piece of data in a new row\n",
    "            f.write(f\"{item_title},{item_site},{news_points},{news_author},{news_time_posted},{news_comments}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py-dm-venv)",
   "language": "python",
   "name": "py-dm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
